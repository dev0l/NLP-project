{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras\n",
    "import string \n",
    "import requests\n",
    "\n",
    "#pip install np_utils\n",
    "#rom tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"\"\"\"Orientation - Terms And Concepts IN System Development \n",
    "The purpose of the sub-part is to orientate oneself generally among different different terms and concepts in system development and does not need to be overworked in any way, everyone who has participated and collaborated with their group will be approved in this part. Those who can briefly and in their own words explain a complicated subject in a simple and understandable language are VG candidates. However, it is the ability to analyze and object model the task that will be decisive for the final grade in Introduction to programming.\n",
    "\n",
    "Team Database: This group delves into databases, each member chooses a database to delve into and writes a summary of the database's most important features, e.g. how to download installs and configures, it is possible to.comply within the group with the same database. The group compiles a document and reports the results.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"orientation - terms and concepts in system development \\nthe purpose of the sub-part is to orientate oneself generally among different different terms and concepts in system development and does not need to be overworked in any way, everyone who has participated and collaborated with their group will be approved in this part. those who can briefly and in their own words explain a complicated subject in a simple and understandable language are vg candidates. however, it is the ability to analyze and object model the task that will be decisive for the final grade in introduction to programming.\\n\\nteam database: this group delves into databases, each member chooses a database to delve into and writes a summary of the database\\'s most important features, e.g. how to download installs and configures, it is possible to.comply within the group with the same database. the group compiles a document and reports the results.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_punctuation(data):\n",
    "    return data.translate(str.maketrans(\"\",\"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " 10,\n",
       " 1,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 13,\n",
       " 2,\n",
       " 25,\n",
       " 14,\n",
       " 2,\n",
       " 26,\n",
       " 15,\n",
       " 7,\n",
       " 4,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 16,\n",
       " 16,\n",
       " 10,\n",
       " 1,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 13,\n",
       " 1,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 4,\n",
       " 8,\n",
       " 34,\n",
       " 3,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 17,\n",
       " 38,\n",
       " 39,\n",
       " 1,\n",
       " 40,\n",
       " 18,\n",
       " 19,\n",
       " 6,\n",
       " 20,\n",
       " 8,\n",
       " 41,\n",
       " 3,\n",
       " 21,\n",
       " 15,\n",
       " 42,\n",
       " 17,\n",
       " 43,\n",
       " 44,\n",
       " 1,\n",
       " 3,\n",
       " 19,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 5,\n",
       " 48,\n",
       " 49,\n",
       " 3,\n",
       " 5,\n",
       " 50,\n",
       " 1,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 22,\n",
       " 7,\n",
       " 2,\n",
       " 57,\n",
       " 4,\n",
       " 58,\n",
       " 1,\n",
       " 59,\n",
       " 60,\n",
       " 2,\n",
       " 61,\n",
       " 62,\n",
       " 20,\n",
       " 8,\n",
       " 63,\n",
       " 64,\n",
       " 2,\n",
       " 65,\n",
       " 66,\n",
       " 3,\n",
       " 67,\n",
       " 4,\n",
       " 68,\n",
       " 69,\n",
       " 9,\n",
       " 21,\n",
       " 6,\n",
       " 70,\n",
       " 23,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 75,\n",
       " 23,\n",
       " 1,\n",
       " 76,\n",
       " 5,\n",
       " 77,\n",
       " 14,\n",
       " 2,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 4,\n",
       " 85,\n",
       " 86,\n",
       " 1,\n",
       " 87,\n",
       " 22,\n",
       " 7,\n",
       " 88,\n",
       " 4,\n",
       " 89,\n",
       " 90,\n",
       " 2,\n",
       " 6,\n",
       " 18,\n",
       " 2,\n",
       " 91,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 92,\n",
       " 5,\n",
       " 93,\n",
       " 1,\n",
       " 94,\n",
       " 2,\n",
       " 95]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllVocabulary Size: 152\n"
     ]
    }
   ],
   "source": [
    "allVocab_size=len(encoded)\n",
    "print('AllVocabulary Size: %d' % allVocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 96\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 151\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "\tsequence = encoded[i-1:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 10],\n",
       " [10, 1],\n",
       " [1, 11],\n",
       " [11, 3],\n",
       " [3, 12],\n",
       " [12, 13],\n",
       " [13, 2],\n",
       " [2, 25],\n",
       " [25, 14],\n",
       " [14, 2],\n",
       " [2, 26],\n",
       " [26, 15],\n",
       " [15, 7],\n",
       " [7, 4],\n",
       " [4, 27],\n",
       " [27, 28],\n",
       " [28, 29],\n",
       " [29, 30],\n",
       " [30, 16],\n",
       " [16, 16],\n",
       " [16, 10],\n",
       " [10, 1],\n",
       " [1, 11],\n",
       " [11, 3],\n",
       " [3, 12],\n",
       " [12, 13],\n",
       " [13, 1],\n",
       " [1, 31],\n",
       " [31, 32],\n",
       " [32, 33],\n",
       " [33, 4],\n",
       " [4, 8],\n",
       " [8, 34],\n",
       " [34, 3],\n",
       " [3, 35],\n",
       " [35, 36],\n",
       " [36, 37],\n",
       " [37, 17],\n",
       " [17, 38],\n",
       " [38, 39],\n",
       " [39, 1],\n",
       " [1, 40],\n",
       " [40, 18],\n",
       " [18, 19],\n",
       " [19, 6],\n",
       " [6, 20],\n",
       " [20, 8],\n",
       " [8, 41],\n",
       " [41, 3],\n",
       " [3, 21],\n",
       " [21, 15],\n",
       " [15, 42],\n",
       " [42, 17],\n",
       " [17, 43],\n",
       " [43, 44],\n",
       " [44, 1],\n",
       " [1, 3],\n",
       " [3, 19],\n",
       " [19, 45],\n",
       " [45, 46],\n",
       " [46, 47],\n",
       " [47, 5],\n",
       " [5, 48],\n",
       " [48, 49],\n",
       " [49, 3],\n",
       " [3, 5],\n",
       " [5, 50],\n",
       " [50, 1],\n",
       " [1, 51],\n",
       " [51, 52],\n",
       " [52, 53],\n",
       " [53, 54],\n",
       " [54, 55],\n",
       " [55, 56],\n",
       " [56, 22],\n",
       " [22, 7],\n",
       " [7, 2],\n",
       " [2, 57],\n",
       " [57, 4],\n",
       " [4, 58],\n",
       " [58, 1],\n",
       " [1, 59],\n",
       " [59, 60],\n",
       " [60, 2],\n",
       " [2, 61],\n",
       " [61, 62],\n",
       " [62, 20],\n",
       " [20, 8],\n",
       " [8, 63],\n",
       " [63, 64],\n",
       " [64, 2],\n",
       " [2, 65],\n",
       " [65, 66],\n",
       " [66, 3],\n",
       " [3, 67],\n",
       " [67, 4],\n",
       " [4, 68],\n",
       " [68, 69],\n",
       " [69, 9],\n",
       " [9, 21],\n",
       " [21, 6],\n",
       " [6, 70],\n",
       " [70, 23],\n",
       " [23, 71],\n",
       " [71, 72],\n",
       " [72, 73],\n",
       " [73, 74],\n",
       " [74, 5],\n",
       " [5, 9],\n",
       " [9, 4],\n",
       " [4, 75],\n",
       " [75, 23],\n",
       " [23, 1],\n",
       " [1, 76],\n",
       " [76, 5],\n",
       " [5, 77],\n",
       " [77, 14],\n",
       " [14, 2],\n",
       " [2, 78],\n",
       " [78, 79],\n",
       " [79, 80],\n",
       " [80, 81],\n",
       " [81, 82],\n",
       " [82, 83],\n",
       " [83, 84],\n",
       " [84, 4],\n",
       " [4, 85],\n",
       " [85, 86],\n",
       " [86, 1],\n",
       " [1, 87],\n",
       " [87, 22],\n",
       " [22, 7],\n",
       " [7, 88],\n",
       " [88, 4],\n",
       " [4, 89],\n",
       " [89, 90],\n",
       " [90, 2],\n",
       " [2, 6],\n",
       " [6, 18],\n",
       " [18, 2],\n",
       " [2, 91],\n",
       " [91, 9],\n",
       " [9, 2],\n",
       " [2, 6],\n",
       " [6, 92],\n",
       " [92, 5],\n",
       " [5, 93],\n",
       " [93, 1],\n",
       " [1, 94],\n",
       " [94, 2],\n",
       " [2, 95]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences=array(sequences)\n",
    "X,y= sequences[:,0],sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24, 10,  1, 11,  3]), array([10,  1, 11,  3, 12]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5],y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encode outputs\n",
    "y= to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             960       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 96)                4896      \n",
      "=================================================================\n",
      "Total params: 18,056\n",
      "Trainable params: 18,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "\tin_text, result = seed_text, seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = array(encoded)\n",
    "\t\t# predict a word in the vocabulary\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text, result = out_word, result + '  ' + out_word\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohamedBabikerMohame\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development  words  simple  writes  not  installs  complicated\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, tokenizer,\"development\", 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\",\n",
    "    \"That which does not kill us makes us stronger.\",\n",
    "    \"I'm not upset that you lied to me, I'm upset that from now on I can't believe you.\",\n",
    "    \"And those who were seen dancing were thought to be insane by those who could not hear the music.\",\n",
    "    \"It is hard enough to remember my opinions, without also remembering my reasons for them!\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is not a lack of love, but a lack of \n",
      "and  words  simple  writes  not\n",
      "\n",
      "that which does not kill us makes us str\n",
      "and  words  simple  writes  not\n",
      "\n",
      "i'm not upset that you lied to me, i'm u\n",
      "and  words  simple  writes  not\n",
      "\n",
      "and those who were seen dancing were tho\n",
      "and  words  simple  writes  not\n",
      "\n",
      "it is hard enough to remember my opinion\n",
      "and  words  simple  writes  not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in data:\n",
    "    seq = w[:40].lower()\n",
    "    print(seq)\n",
    "    print(generate_seq(model, tokenizer,\"and\", 4))\n",
    "   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
